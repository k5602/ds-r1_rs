{
  "vocab_size": 32000,
  "hidden_size": 512,
  "num_layers": 8,
  "num_heads": 8,
  "intermediate_size": 2048,
  "max_seq_len": 2048,
  "attention_type": "Standard",
  "ff_type": "MoE",
  "mla_every": null,
  "moe_every": null,
  "kv_compression_ratio": 0.5,
  "rope_theta": 10000.0,
  "num_experts": 8,
  "experts_per_token": 2,
  "thinking_token_id": 32001,
  "max_reasoning_steps": 256,
  "dropout_prob": 0.1,
  "layer_norm_eps": 1e-5
}
